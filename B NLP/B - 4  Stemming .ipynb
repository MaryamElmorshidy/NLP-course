{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running --> run\n",
      "ran --> ran\n",
      "early --> earli\n",
      "played --> play\n",
      "coding --> code\n",
      "national --> nation\n",
      "natural --> natur\n",
      "exactly --> exactli\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.porter import *\n",
    "p_stemmer = PorterStemmer()\n",
    "word = [\"running\",\"ran\",\"early\",\"played\",\"coding\",\"national\",\"natural\",\"exactly\"]\n",
    "for w in word:\n",
    "    print(w,\"-->\",p_stemmer.stem(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running --> run\n",
      "ran --> ran\n",
      "early --> earli\n",
      "played --> play\n",
      "coding --> code\n",
      "national --> nation\n",
      "natural --> natur\n",
      "exactly --> exact\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.snowball import *\n",
    "s_stemmer=SnowballStemmer(language=\"english\")\n",
    "word = [\"running\",\"ran\",\"early\",\"played\",\"coding\",\"national\",\"natural\",\"exactly\"]\n",
    "for w in word:\n",
    "    print(w,\"-->\",s_stemmer.stem(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer,LancasterStemmer\n",
    "from nltk.tokenize import word_tokenize,sent_tokenize\n",
    "\n",
    "ps=PorterStemmer()\n",
    "ls=LancasterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "was --> wa\n",
      "was --> was\n",
      "-------------------------------------\n",
      "were --> were\n",
      "were --> wer\n",
      "-------------------------------------\n",
      "been --> been\n",
      "been --> been\n",
      "-------------------------------------\n",
      "be --> be\n",
      "be --> be\n",
      "-------------------------------------\n",
      "being --> be\n",
      "being --> being\n",
      "-------------------------------------\n",
      "is --> is\n",
      "is --> is\n",
      "-------------------------------------\n",
      "are --> are\n",
      "are --> ar\n",
      "-------------------------------------\n"
     ]
    }
   ],
   "source": [
    "word=[\"was\",\"were\",\"been\",\"be\",\"being\",\"is\",\"are\"]\n",
    "for w in word:\n",
    "    print(w,\"-->\",ps.stem(w))\n",
    "    print(w,\"-->\",ls.stem(w))\n",
    "    print(\"-------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp=spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It             PRON      10239237003504588839      it\n",
      "may            AUX       14378475389916013800      may\n",
      "sound          VERB       1289836513871212815      sound\n",
      "strange        ADJ        8371520351987192855      strange\n",
      "to             PART       3791531372978436496      to\n",
      "voluntarily    ADV         917427667341859414      voluntarily\n",
      "set            VERB      11268780503345276587      set\n",
      "aside          ADP       11882247414655843586      aside\n",
      "part           NOUN       4485934323942657167      part\n",
      "of             ADP         886050111519832510      of\n",
      "the            DET        7425985699627899538      the\n",
      "data           NOUN       8931270445620108520      datum\n",
      "at             ADP       11667289587015813222      at\n",
      "this           DET        1995909169258310477      this\n",
      "stage          NOUN       8764522039650230071      stage\n",
      ".              PUNCT     12646065887601541794      .\n",
      "After          ADV       13428508259213873547      after\n",
      "all            ADV       13409319323822384369      all\n",
      ",              PUNCT      2593208677638477497      ,\n",
      "\n",
      "              SPACE       962983613142996970      \n",
      "\n",
      "you            PRON       7624161793554793053      you\n",
      "have           AUX       14692702688101715474      have\n",
      "only           ADV       13398675276606405380      only\n",
      "taken          VERB       6789454535283781228      take\n",
      "a              DET       11901859001352538922      a\n",
      "quick          ADJ       12442504647632856847      quick\n",
      "glance         NOUN       2925608815477570901      glance\n",
      "at             ADP       11667289587015813222      at\n",
      "the            DET        7425985699627899538      the\n",
      "data           NOUN       8931270445620108520      datum\n",
      ",              PUNCT      2593208677638477497      ,\n",
      "and            CCONJ      2283656566040971221      and\n",
      "surely         ADV       15302800381503722267      surely\n",
      "you            PRON       7624161793554793053      you\n",
      "should         AUX       10292920167869855674      should\n",
      "learn          VERB       9664905639869093544      learn\n",
      "a              DET       11901859001352538922      a\n",
      "whole          ADJ       16948554243429412012      whole\n",
      "\n",
      "              SPACE       962983613142996970      \n",
      "\n",
      "lot            NOUN       6920515201346452032      lot\n",
      "more           ADJ        2160362229054775535      more\n",
      "about          ADP         942632335873952620      about\n",
      "it             PRON      10239237003504588839      it\n",
      "before         ADP       11320251846592927908      before\n",
      "you            PRON       7624161793554793053      you\n",
      "decide         VERB       3634000357447379468      decide\n",
      "what           PRON       5865838185239622912      what\n",
      "algorithms     NOUN       7002907533488190321      algorithm\n",
      "to             PART       3791531372978436496      to\n",
      "use            VERB       6873750497785110593      use\n",
      ",              PUNCT      2593208677638477497      ,\n",
      "right          ADJ        5943797630011647483      right\n",
      "?              PUNCT      8205403955989537350      ?\n",
      "This           DET        1995909169258310477      this\n",
      "is             AUX       10382539506755952630      be\n",
      "true           ADJ        7434368892455186804      true\n",
      ",              PUNCT      2593208677638477497      ,\n",
      "but            CCONJ     14560795576765492085      but\n",
      "\n",
      "              SPACE       962983613142996970      \n",
      "\n",
      "your           PRON       1572612192562026184      your\n",
      "brain          NOUN       3746349434424197164      brain\n",
      "is             VERB      10382539506755952630      be\n",
      "an             DET       15099054000809333061      an\n",
      "amazing        ADJ       12968186374132960503      amazing\n",
      "pattern        NOUN      15329811787164753587      pattern\n",
      "detection      NOUN       7830947080694346521      detection\n",
      "system         NOUN      16001232513058686804      system\n"
     ]
    }
   ],
   "source": [
    "doc1=nlp(\"\"\"It may sound strange to voluntarily set aside part of the data at this stage. After all,\n",
    "you have only taken a quick glance at the data, and surely you should learn a whole\n",
    "lot more about it before you decide what algorithms to use, right? This is true, but\n",
    "your brain is an amazing pattern detection system\"\"\")\n",
    "\n",
    "for w in doc1:\n",
    "    print(f\"{w.text :{15}}{w.pos_ :{10}}{w.lemma :{20}}      {w.lemma_ }\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmashow (doc):  \n",
    "    for w in doc:\n",
    "        print(f\"{w.text :{15}}{w.pos_ :{10}}{w.lemma_ }\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i              PRON      I\n",
      "was            AUX       be\n",
      "very           ADV       very\n",
      "happy          ADJ       happy\n",
      "for            ADP       for\n",
      "meeting        NOUN      meeting\n",
      ".              PUNCT     .\n"
     ]
    }
   ],
   "source": [
    "doc2=nlp(\"i was very happy for meeting . \")\n",
    "lemmashow(doc2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running --> running\n",
      "ran --> ran\n",
      "early --> early\n",
      "played --> played\n",
      "coding --> coding\n",
      "national --> national\n",
      "natural --> natural\n",
      "exactly --> exactly\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "w_stemmmer=WordNetLemmatizer()\n",
    "\n",
    "word = [\"running\",\"ran\",\"early\",\"played\",\"coding\",\"national\",\"natural\",\"exactly\"]\n",
    "for w in word:\n",
    "    print(w,\"-->\",w_stemmmer.lemmatize(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running --w_stemmmer--> running\n",
      "running --s_stemmmer--> run\n",
      "running --p_stemmmer--> run\n",
      "--------------------------------------------------\n",
      "ran --w_stemmmer--> ran\n",
      "ran --s_stemmmer--> ran\n",
      "ran --p_stemmmer--> ran\n",
      "--------------------------------------------------\n",
      "early --w_stemmmer--> early\n",
      "early --s_stemmmer--> earli\n",
      "early --p_stemmmer--> earli\n",
      "--------------------------------------------------\n",
      "played --w_stemmmer--> played\n",
      "played --s_stemmmer--> play\n",
      "played --p_stemmmer--> play\n",
      "--------------------------------------------------\n",
      "coding --w_stemmmer--> coding\n",
      "coding --s_stemmmer--> code\n",
      "coding --p_stemmmer--> code\n",
      "--------------------------------------------------\n",
      "national --w_stemmmer--> national\n",
      "national --s_stemmmer--> nation\n",
      "national --p_stemmmer--> nation\n",
      "--------------------------------------------------\n",
      "natural --w_stemmmer--> natural\n",
      "natural --s_stemmmer--> natur\n",
      "natural --p_stemmmer--> natur\n",
      "--------------------------------------------------\n",
      "exactly --w_stemmmer--> exactly\n",
      "exactly --s_stemmmer--> exact\n",
      "exactly --p_stemmmer--> exactli\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "word = [\"running\",\"ran\",\"early\",\"played\",\"coding\",\"national\",\"natural\",\"exactly\"]\n",
    "for w in word:\n",
    "    print(w,\"--w_stemmmer-->\",w_stemmmer.lemmatize(w))\n",
    "    print(w,\"--s_stemmmer-->\",s_stemmer.stem(w))\n",
    "    print(w,\"--p_stemmmer-->\",p_stemmer.stem(w))\n",
    "    print(\"--------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "meeting\n",
      "meet\n"
     ]
    }
   ],
   "source": [
    "print(w_stemmmer.lemmatize(\"meeting\",\"n\"))\n",
    "print(w_stemmmer.lemmatize(\"meeting\",\"v\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['He', 'was', 'running', 'and', 'eating', 'at', 'same', 'time', 'He', 'has', 'bad', 'habit', 'of', 'swimming', 'after', 'playing', 'long', 'hours', 'in', 'the', 'Sun']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "w_stemmmer=WordNetLemmatizer()\n",
    "sentence = \"He was running and eating at same time. He has bad habit of swimming after playing long hours in the Sun.\"\n",
    "punctuations=\"?:!.,;\"\n",
    "token_sent=word_tokenize(sentence)\n",
    "for w in token_sent :\n",
    "    if w in punctuations:\n",
    "        token_sent.remove(w)\n",
    "print(token_sent   )     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word           lemma          \n",
      "He             He             \n",
      "was            wa             \n",
      "running        running        \n",
      "and            and            \n",
      "eating         eating         \n",
      "at             at             \n",
      "same           same           \n",
      "time           time           \n",
      "He             He             \n",
      "has            ha             \n",
      "bad            bad            \n",
      "habit          habit          \n",
      "of             of             \n",
      "swimming       swimming       \n",
      "after          after          \n",
      "playing        playing        \n",
      "long           long           \n",
      "hours          hour           \n",
      "in             in             \n",
      "the            the            \n",
      "Sun            Sun            \n"
     ]
    }
   ],
   "source": [
    "print(\"{0:15}{1:15}\".format(\"word\",\"lemma\"))\n",
    "for w in token_sent:\n",
    "    print(\"{0:15}{1:15}\".format(w,w_stemmmer.lemmatize(w)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word           lemma          \n",
      "He             He             \n",
      "was            be             \n",
      "running        run            \n",
      "and            and            \n",
      "eating         eat            \n",
      "at             at             \n",
      "same           same           \n",
      "time           time           \n",
      "He             He             \n",
      "has            have           \n",
      "bad            bad            \n",
      "habit          habit          \n",
      "of             of             \n",
      "swimming       swim           \n",
      "after          after          \n",
      "playing        play           \n",
      "long           long           \n",
      "hours          hours          \n",
      "in             in             \n",
      "the            the            \n",
      "Sun            Sun            \n"
     ]
    }
   ],
   "source": [
    "print(\"{0:15}{1:15}\".format(\"word\",\"lemma\"))\n",
    "for w in token_sent:\n",
    "    print(\"{0:15}{1:15}\".format(w,w_stemmmer.lemmatize(w,pos=\"v\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "لعب --w_stemmmer--> لعب\n",
      "لعب --s_stemmmer--> لعب\n",
      "لعب --p_stemmmer--> لعب\n",
      "--------------------------------------------------\n",
      "نلعب --w_stemmmer--> نلعب\n",
      "نلعب --s_stemmmer--> نلعب\n",
      "نلعب --p_stemmmer--> نلعب\n",
      "--------------------------------------------------\n",
      "لعبوا --w_stemmmer--> لعبوا\n",
      "لعبوا --s_stemmmer--> لعب\n",
      "لعبوا --p_stemmmer--> لعبوا\n",
      "--------------------------------------------------\n",
      "لعبن --w_stemmmer--> لعبن\n",
      "لعبن --s_stemmmer--> لعب\n",
      "لعبن --p_stemmmer--> لعبن\n",
      "--------------------------------------------------\n",
      "يلعبون --w_stemmmer--> يلعبون\n",
      "يلعبون --s_stemmmer--> يلعب\n",
      "يلعبون --p_stemmmer--> يلعبون\n",
      "--------------------------------------------------\n",
      "تلعبن --w_stemmmer--> تلعبن\n",
      "تلعبن --s_stemmmer--> تلعب\n",
      "تلعبن --p_stemmmer--> تلعبن\n",
      "--------------------------------------------------\n",
      "اللعب --w_stemmmer--> اللعب\n",
      "اللعب --s_stemmmer--> لعب\n",
      "اللعب --p_stemmmer--> اللعب\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "word=[\"لعب\",\"نلعب\",\"لعبوا\",\"لعبن\",\"يلعبون\",\"تلعبن\",\"اللعب\"]\n",
    "s_a_stemmer=SnowballStemmer(language=\"arabic\")\n",
    "for w in word:\n",
    "    print(w,\"--w_stemmmer-->\",w_stemmmer.lemmatize(w))\n",
    "    print(w,\"--s_stemmmer-->\",s_a_stemmer.stem(w))\n",
    "    print(w,\"--p_stemmmer-->\",p_stemmer.stem(w))\n",
    "    print(\"--------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               SPACE      8532415787641010193       \n",
      "يلعبون         NOUN       3244542094178779093      يلعبون\n",
      "لعب            NOUN      15074585553970399291      لعب\n",
      "لعبوا          PROPN     15388641861175024868      لعبوا\n",
      "لعبن           PROPN     15019117817111304837      لعبن\n",
      "اللعب          PROPN     16044272878393491160      اللعب\n",
      "تلعبن          VERB      11736769596647488367      تلعبن\n"
     ]
    }
   ],
   "source": [
    "doc3=nlp(\" يلعبون لعب لعبوا لعبن اللعب تلعبن\")\n",
    "for w in doc3:\n",
    "    print(f\"{w.text :{15}}{w.pos_ :{10}}{w.lemma :{20}}      {w.lemma_ }\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
